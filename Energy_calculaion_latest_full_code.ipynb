{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a679fe4d-375e-49e1-9b00-f8169831f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Data manipulation and analysis\n",
    "import numpy as np  # Numerical computations and array manipulations\n",
    "import matplotlib.pyplot as plt  # Data visualization and plotting\n",
    "import seaborn as sns  # Statistical data visualization\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  # Splitting data and hyperparameter tuning\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  # Model evaluation metrics\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor  # Regression models\n",
    "from sklearn.svm import SVR  # Support Vector Regression model\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder  # Preprocessing for numerical and categorical data\n",
    "from sklearn.compose import ColumnTransformer  # Handling transformations for mixed data types\n",
    "from sklearn.pipeline import Pipeline  # Creating pipelines for preprocessing and modeling\n",
    "from datetime import datetime  # Working with date and time\n",
    "from sklearn.feature_selection import SelectKBest, f_regression  # Feature selection methods\n",
    "import scipy.stats as stats  # Statistical functions\n",
    "\n",
    "# Load dataset\n",
    "energy_data = pd.read_csv(\"energydata_complete.csv\")  # Loading the dataset to analyze energy consumption patterns\n",
    "\n",
    "# Convert date column to datetime\n",
    "energy_data['date'] = pd.to_datetime(energy_data['date'], format='%Y-%m-%d %H:%M:%S')  # Converting date column to datetime format\n",
    "\n",
    "# Add features\n",
    "energy_data['NSM'] = energy_data['date'].dt.hour * 3600 + energy_data['date'].dt.minute * 60 + energy_data['date'].dt.second  # Calculating seconds since midnight to capture time-based trends\n",
    "energy_data['WeekStatus'] = energy_data['date'].dt.weekday.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')  # Classifying days as weekdays or weekends\n",
    "energy_data['Day_of_week'] = energy_data['date'].dt.day_name()  # Extracting the name of the day to analyze patterns by day\n",
    "\n",
    "\n",
    "# Drop unused columns\n",
    "energy_data = energy_data.drop(['date'], axis=1)  # Dropping the original date column as it's no longer needed\n",
    "\n",
    "# Splitting the dataset\n",
    "X = energy_data.drop('Appliances', axis=1)  # Defining features\n",
    "y = energy_data['Appliances']  # Defining target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)  # Splitting data into training and testing sets\n",
    "\n",
    "# Preprocessing pipeline\n",
    "categorical_features = ['WeekStatus', 'Day_of_week']  # List of categorical features\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]  # Identifying numerical features\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),  # Standardizing numerical features\n",
    "        ('cat', OneHotEncoder(), categorical_features)  # Encoding categorical features using one-hot encoding\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define models with hyperparameter tuning\n",
    "param_grid = {\n",
    "    'GradientBoosting': {\n",
    "        'model__n_estimators': [100, 200, 300],  # Number of boosting stages\n",
    "        \n",
    "        'model__max_depth': [3, 5, 7],  # Depth of trees\n",
    "        'model__learning_rate': [0.05, 0.1, 0.2]  # Learning rate for boosting\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model__n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "        'model__max_depth': [5, 10, 15]  # Maximum depth of trees\n",
    "    },\n",
    "    'SVR': {\n",
    "        'model__C': [0.1, 1, 10],  # Regularization parameter\n",
    "        'model__epsilon': [0.01, 0.1, 0.5]  # Epsilon-tube within which predictions are considered correct\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "results = {}  # Dictionary to store results for each model\n",
    "for name, param in param_grid.items():\n",
    "    # Initialize the model based on its name\n",
    "    if name == 'GradientBoosting':\n",
    "        model = GradientBoostingRegressor()\n",
    "    elif name == 'RandomForest':\n",
    "        model = RandomForestRegressor()\n",
    "    else:\n",
    "        model = SVR(kernel='rbf')\n",
    "\n",
    "    # Create a pipeline for preprocessing, feature selection, and modeling\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),  # Preprocess data (scaling and encoding)\n",
    "        ('selector', SelectKBest(score_func=f_regression, k=10)),  # Feature selection after preprocessing\n",
    "        ('model', model)  # Apply the regression model\n",
    "    ])\n",
    "    # Perform grid search for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(X_train, y_train)  # Fit the model using grid search\n",
    "    y_pred = grid_search.best_estimator_.predict(X_test)  # Make predictions on test data\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # Root Mean Squared Error\n",
    "    mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n",
    "    r2 = r2_score(y_test, y_pred)  # R-squared score\n",
    "\n",
    "    results[name] = {'RMSE': rmse, 'MAE': mae, 'R2': r2}  # Store results\n",
    "    print(f\"{name} - Best Params: {grid_search.best_params_} - RMSE: {rmse}, MAE: {mae}, R2: {r2}\")  # Print results\n",
    "\n",
    "# Save actual vs predicted results\n",
    "predictions['Actual'] = y_test.values\n",
    "predictions.to_csv('predictions.csv', index=False)  # Save predictions to CSV\n",
    "\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 5))  # Create subplots for metrics visualization\n",
    "for i, (name, metrics) in enumerate(results.items()):\n",
    "    sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), ax=ax[i])  # Plot metrics for each model\n",
    "    ax[i].set_title(name)  # Add title for each plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix and heatmap\n",
    "X_encoded = pd.get_dummies(X_train, columns=['WeekStatus', 'Day_of_week'])  # Encode categorical variables\n",
    "corr_matrix = X_train.corr()  # Calculate correlations between features\n",
    "plt.figure(figsize=(12, 8))  # Set plot size\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')  # Plot correlation matrix as a heatmap\n",
    "plt.title('Correlation Matrix')  # Add title\n",
    "plt.show()\n",
    "\n",
    "# Save training and testing sets\n",
    "X_train.to_csv('training.csv', index=False)  # Save training set to CSV file\n",
    "X_test.to_csv('testing.csv', index=False)  # Save testing set to CSV file\n",
    "\n",
    "X_encoded = pd.get_dummies(X_train, columns=['WeekStatus', 'Day_of_week'])  # Encode categorical variables\n",
    "corr_matrix = X_encoded.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Save datasets\n",
    "X_train.to_csv('training.csv', index=False)\n",
    "X_test.to_csv('testing.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bff35a-cbf8-4424-afb3-11f2028baee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df1eb4c-4160-4b51-9f0b-9492fd0aa211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
